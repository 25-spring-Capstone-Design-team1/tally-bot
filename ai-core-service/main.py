import json
from fastapi import HTTPException, BackgroundTasks, Request
import hashlib
import time
from typing import Dict, Any

from config.app_config import create_app
from config.service_config import ensure_api_key, get_api_keys
from models.conversation import ConversationRequest, ConversationResponse, EvaluationRequest
from handlers.process_handler import (
    process_conversation_logic,
    load_resources,
    process_conversation_with_sequential_chain,
    process_conversation_with_simplified_chain
)
# ìƒˆë¡œìš´ í‰ê°€ ìœ í‹¸ë¦¬í‹° import
from utils.settlement_evaluator import evaluate_settlement_results
from utils.advanced_metrics import evaluate_advanced_metrics

# FastAPI ì•± ìƒì„±
app = create_app()

# ì¤‘ë³µ ìš”ì²­ ë°©ì§€ë¥¼ ìœ„í•œ ì§„í–‰ì¤‘ ìš”ì²­ ì¶”ì 
in_progress_requests: Dict[str, float] = {}
REQUEST_TIMEOUT = 60  # 60ì´ˆ í›„ ì§„í–‰ì¤‘ ìš”ì²­ ë§Œë£Œ

def generate_request_hash(request: ConversationRequest) -> str:
    """ìš”ì²­ì˜ ê³ ìœ  í•´ì‹œë¥¼ ìƒì„±í•©ë‹ˆë‹¤"""
    # ì¤‘ìš”í•œ í•„ë“œë“¤ë§Œ ì‚¬ìš©í•´ì„œ í•´ì‹œ ìƒì„±
    hash_data = {
        "chatroom_name": request.chatroom_name,
        "members": request.members,
        "messages": [{"speaker": msg.speaker, "content": msg.message_content} for msg in request.messages[-5:]]  # ë§ˆì§€ë§‰ 5ê°œ ë©”ì‹œì§€ë§Œ
    }
    hash_string = json.dumps(hash_data, sort_keys=True, ensure_ascii=False)
    return hashlib.md5(hash_string.encode()).hexdigest()

def cleanup_expired_requests():
    """ë§Œë£Œëœ ì§„í–‰ì¤‘ ìš”ì²­ë“¤ì„ ì •ë¦¬í•©ë‹ˆë‹¤"""
    current_time = time.time()
    expired_keys = [key for key, timestamp in in_progress_requests.items() 
                   if current_time - timestamp > REQUEST_TIMEOUT]
    for key in expired_keys:
        del in_progress_requests[key]
    
    # ì§„í–‰ì¤‘ ìš”ì²­ ìƒíƒœ ë¡œê¹…
    if expired_keys:
        print(f"ğŸ—‘ï¸ ë§Œë£Œëœ ì§„í–‰ì¤‘ ìš”ì²­ {len(expired_keys)}ê°œ ì‚­ì œ, í˜„ì¬ ì§„í–‰ì¤‘: {len(in_progress_requests)}")

def create_member_mapping(members_data):
    """ë©¤ë²„ ë°ì´í„°ì—ì„œ ID-ì´ë¦„ ë§¤í•‘ì„ ìƒì„±í•©ë‹ˆë‹¤"""
    id_to_name = {}
    name_to_id = {}
    
    for member_dict in members_data:
        for member_id, member_name in member_dict.items():
            id_to_name[member_id] = member_name
            name_to_id[member_name] = member_id
    
    return id_to_name, name_to_id

def convert_members_to_single_object(members_data):
    """
    ë¶„ë¦¬ëœ ë©¤ë²„ ê°ì²´ë“¤ì„ í•˜ë‚˜ì˜ ê°ì²´ë¡œ í•©ì¹©ë‹ˆë‹¤
    ì…ë ¥: [{'8': 'ì´ë‹¤ë¹ˆ'}, {'9': 'ì„ì¬ë¯¼'}, {'10': 'ì •í˜œìœ¤'}, {'11': 'í—ˆì›í˜'}]
    ì¶œë ¥: [{'8': 'ì´ë‹¤ë¹ˆ', '9': 'ì„ì¬ë¯¼', '10': 'ì •í˜œìœ¤', '11': 'í—ˆì›í˜'}]
    """
    if not members_data:
        return []
    
    # ëª¨ë“  ë©¤ë²„ ë”•ì…”ë„ˆë¦¬ë¥¼ í•˜ë‚˜ë¡œ í•©ì¹˜ê¸°
    merged_dict = {}
    for member_dict in members_data:
        merged_dict.update(member_dict)
    
    # í•©ì³ì§„ ë”•ì…”ë„ˆë¦¬ë¥¼ ë°°ì—´ì— ë„£ì–´ì„œ ë°˜í™˜
    return [merged_dict]

@app.get("/", 
         summary="ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸",
         description="API ì„œë¹„ìŠ¤ì˜ ê¸°ë³¸ ìƒíƒœë¥¼ í™•ì¸í•©ë‹ˆë‹¤.",
         tags=["Health Check"])
async def root():
    return {"message": "Tally Bot AI Core Service API"}

@app.post("/api/process", 
          response_model=ConversationResponse,
          summary="ì‹¤ì‹œê°„ ëŒ€í™” ì²˜ë¦¬ (ë‹¨ìˆœí™”ëœ ì²´ì¸)",
          description="""
          hint_phrasesë¥¼ ì§ì ‘ íŒŒì‹±í•˜ëŠ” ë‹¨ìˆœí™”ëœ ì²˜ë¦¬ APIì…ë‹ˆë‹¤.
          
          ### ğŸš€ ê°œì„ ì‚¬í•­
          - final_prompt ì œê±°ë¡œ ì²˜ë¦¬ ì†ë„ í–¥ìƒ
          - LLM í˜¸ì¶œ 3íšŒ â†’ 2íšŒë¡œ ê°ì†Œ
          - hint_phrases ê·œì¹™ ê¸°ë°˜ íŒŒì‹±ìœ¼ë¡œ ì¼ê´€ì„± í–¥ìƒ
          
          ### âš¡ ì²˜ë¦¬ ê³¼ì •
          1. 1ì°¨: ì •ì‚° í•­ëª© ì¶”ì¶œ (hint_phrases í¬í•¨)
          2. 2ì°¨: ì¥ì†Œ ì •ë³´ ì¶”ì¶œ
          3. 3ì°¨: hint_phrases ì§ì ‘ íŒŒì‹± â†’ ì •ì‚° JSON ìƒì„±
          
          ### âœ¨ íŠ¹ì§•
          - ë” ë¹ ë¥¸ ì²˜ë¦¬ ì†ë„
          - ë” ì¼ê´€ëœ ê²°ê³¼
          - ê·œì¹™ ê¸°ë°˜ ì•ˆì •ì„±
          """,
          tags=["Core Processing"])
async def process_api(request: ConversationRequest, background_tasks: BackgroundTasks):
    # ì¤‘ë³µ ìš”ì²­ ì²´í¬ (ì§„í–‰ì¤‘ ìš”ì²­ë§Œ ë°©ì§€, ë§¤ë²ˆ ìƒˆë¡œ ì²˜ë¦¬)
    request_hash = generate_request_hash(request)
    current_time = time.time()
    
    # ë§Œë£Œëœ ì§„í–‰ì¤‘ ìš”ì²­ ì •ë¦¬
    cleanup_expired_requests()
    
    # ì¤‘ë³µ ìš”ì²­ í™•ì¸ (ë™ì‹œì— ê°™ì€ ìš”ì²­ì´ ì²˜ë¦¬ì¤‘ì´ë©´ ê±°ë¶€)
    duplicate_prevention_enabled = True  # ì¤‘ë³µ ë°©ì§€ í™œì„±í™”
    
    if duplicate_prevention_enabled and request_hash in in_progress_requests:
        elapsed_time = current_time - in_progress_requests[request_hash]
        print(f"ğŸ”„ ì¤‘ë³µ ìš”ì²­ ê°ì§€! ì§„í–‰ì¤‘ ìš”ì²­ ì²˜ë¦¬ ì¤‘ (í•´ì‹œ: {request_hash[:8]}, ê²½ê³¼: {elapsed_time:.1f}ì´ˆ)")
        raise HTTPException(
            status_code=429,
            detail=f"ë™ì¼í•œ ìš”ì²­ì´ ì²˜ë¦¬ ì¤‘ì…ë‹ˆë‹¤. {elapsed_time:.1f}ì´ˆ ê²½ê³¼, ì ì‹œ ëŒ€ê¸°í•´ì£¼ì„¸ìš”."
        )
    
    # ì§„í–‰ì¤‘ ìš”ì²­ìœ¼ë¡œ ë“±ë¡
    if duplicate_prevention_enabled:
        in_progress_requests[request_hash] = current_time
        print(f"ğŸš€ ìƒˆ ìš”ì²­ ì²˜ë¦¬ ì‹œì‘ (í•´ì‹œ: {request_hash[:8]})")
    
    try:
        # ===== ì…ë ¥ JSON ê²€ì¦ =====
        print("ğŸ” === ë‹¨ìˆœí™”ëœ ì²´ì¸ ì…ë ¥ JSON ê²€ì¦ ===")
        print(f"ìš”ì²­ í•´ì‹œ: {request_hash[:8]} (ì¤‘ë³µë°©ì§€: {'í™œì„±í™”' if duplicate_prevention_enabled else 'ë¹„í™œì„±í™”'})")
        print(f"ì±„íŒ…ë°© ì´ë¦„: {request.chatroom_name}")
        print(f"ì›ë³¸ ë©¤ë²„ ìˆ˜: {len(request.members)}")
        print(f"ì›ë³¸ ë©¤ë²„ ë°ì´í„°: {request.members}")
        
        # ë©¤ë²„ ë°ì´í„° í˜•ì‹ ë³€í™˜: ë¶„ë¦¬ëœ ê°ì²´ë“¤ â†’ ë‹¨ì¼ ê°ì²´
        converted_members = convert_members_to_single_object(request.members)
        print(f"ë³€í™˜ëœ ë©¤ë²„ ë°ì´í„°: {converted_members}")
        
        print(f"ë©”ì‹œì§€ ìˆ˜: {len(request.messages)}")
        print(f"ì²« ë²ˆì§¸ ë©”ì‹œì§€: speaker={request.messages[0].speaker}, content='{request.messages[0].message_content}'")
        print(f"ë§ˆì§€ë§‰ ë©”ì‹œì§€: speaker={request.messages[-1].speaker}, content='{request.messages[-1].message_content}'")
        print("ğŸ” =======================================\n")
        
        # í”„ë¡¬í”„íŠ¸ ë¡œë“œ (final_promptëŠ” ì‚¬ìš©í•˜ì§€ ì•Šì§€ë§Œ í˜¸í™˜ì„±ì„ ìœ„í•´ ë¡œë“œ)
        input_prompt, secondary_prompt, final_prompt, _ = await load_resources(
            request.prompt_file,
            request.secondary_prompt_file,
            request.final_prompt_file
        )
        
        # ë³€í™˜ëœ ë©¤ë²„ ë°ì´í„°ë¡œ ID-ì´ë¦„ ë§¤í•‘ ìƒì„±
        id_to_name, name_to_id = create_member_mapping(converted_members)
        
        # sample_conversation.json í˜•ì‹ì—ì„œ í•„ìš”í•œ ëŒ€í™” í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        conversation = [{
            'speaker': 'system', 
            'message_content': f"member_count: {len(id_to_name)}\nmember_mapping: {json.dumps(id_to_name, ensure_ascii=False)}"
        }]
        
        # ì‹¤ì œ ëŒ€í™” ë‚´ìš© ì¶”ê°€
        conversation.extend([
            {
                'speaker': msg.speaker,
                'message_content': msg.message_content
            }
            for msg in request.messages
        ])
        
        # ===== AIì—ê²Œ ì „ë‹¬ë˜ëŠ” ìµœì¢… ëŒ€í™” ë°ì´í„° ë¡œê¹… =====
        print("ğŸ¤– === AIì—ê²Œ ì „ë‹¬ë˜ëŠ” ìµœì¢… ëŒ€í™” ë°ì´í„° ===")
        print(f"ì „ì²´ ëŒ€í™” ê¸¸ì´: {len(conversation)}")
        print(f"ì‹œìŠ¤í…œ ë©”ì‹œì§€: {conversation[0]}")
        print("ì‹¤ì œ ëŒ€í™” ë‚´ìš©:")
        for i, msg in enumerate(conversation[1:], 1):
            print(f"  [{i}] {msg['speaker']}: {msg['message_content']}")
        print("ğŸ¤– ============================================\n")
        
        # ëŒ€í™” ê¸¸ì´ í™•ì¸ ë° ì²­í¬ ì²˜ë¦¬ ì˜µì…˜ ì„¤ì •
        use_chunking = len(request.messages) > 15
        
        # ë‹¨ìˆœí™”ëœ ì²´ì¸ ì²˜ë¦¬ ë¡œì§ í˜¸ì¶œ (final_prompt ì‚¬ìš© ì•ˆí•¨)
        result = await process_conversation_with_simplified_chain(
            conversation=conversation,
            input_prompt=input_prompt,
            secondary_prompt=secondary_prompt,
            member_names=list(id_to_name.values()),
            id_to_name=id_to_name,
            name_to_id=name_to_id,
            use_chunking=use_chunking
        )
        
        print(f"âœ… ìš”ì²­ ì²˜ë¦¬ ì™„ë£Œ (í•´ì‹œ: {request_hash[:8]})")
        return result
    
    except Exception as e:
        print(f"âŒ ìš”ì²­ ì²˜ë¦¬ ì‹¤íŒ¨ (í•´ì‹œ: {request_hash[:8]}): {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error: {str(e)}")
    
    finally:
        # ì²˜ë¦¬ ì™„ë£Œ í›„ ì§„í–‰ì¤‘ ìš”ì²­ì—ì„œ ì œê±°
        if duplicate_prevention_enabled and request_hash in in_progress_requests:
            del in_progress_requests[request_hash]
            print(f"ğŸ ìš”ì²­ ì™„ë£Œ, ì§„í–‰ì¤‘ ëª©ë¡ì—ì„œ ì œê±° (í•´ì‹œ: {request_hash[:8]})")

@app.post("/api/process-file",
          summary="íŒŒì¼ ê¸°ë°˜ ëŒ€í™” ì²˜ë¦¬",
          description="""
          JSON íŒŒì¼ì— ì €ì¥ëœ ëŒ€í™” ë°ì´í„°ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.
          
          ### ğŸ“‚ íŒŒì¼ í˜•ì‹
          - sample_conversation.json í˜•ì‹ ì§€ì›
          - ë©¤ë²„ ì •ë³´ì™€ ëŒ€í™” ë‚´ìš© í¬í•¨
          
          ### âš™ï¸ ì„¤ì • ì˜µì…˜
          - í”„ë¡¬í”„íŠ¸ íŒŒì¼ ê²½ë¡œ ì„¤ì •
          - ì²­í‚¹ ì²˜ë¦¬ í™œì„±í™”/ë¹„í™œì„±í™”
          """,
          tags=["File Processing"])
async def process_conversation_from_file(
    background_tasks: BackgroundTasks,
    conversation_file: str = "resources/sample_conversation.json", # ëŒ€í™” JSON íŒŒì¼
    prompt_file: str = "resources/input_prompt.yaml", # 1ì°¨ í”„ë¡¬í”„íŠ¸ íŒŒì¼
    secondary_prompt_file: str = "resources/secondary_prompt.yaml",  # 2ì°¨ í”„ë¡¬í”„íŠ¸ íŒŒì¼
    final_prompt_file: str = "resources/final_prompt.yaml",  # 3ì°¨ í”„ë¡¬í”„íŠ¸ íŒŒì¼
    use_chunking: bool = True  # ì²­í¬ ì²˜ë¦¬ ì‚¬ìš© ì—¬ë¶€
):
    try:
        # í”„ë¡¬í”„íŠ¸ì™€ ëŒ€í™” ë¡œë“œ
        input_prompt, secondary_prompt, final_prompt, conversation = await load_resources(
            prompt_file,
            secondary_prompt_file,
            final_prompt_file,
            conversation_file
        )
        
        # member ì •ë³´ ì¶”ì¶œ ë° ë§¤í•‘ ìƒì„±
        members_text = conversation[0]['message_content']
        
        # ID-ì´ë¦„ ë§¤í•‘ì´ ì´ë¯¸ ìˆëŠ”ì§€ í™•ì¸
        if 'member_mapping:' in members_text:
            # ê¸°ì¡´ ë§¤í•‘ ì •ë³´ ì‚¬ìš©
            mapping_line = [line for line in members_text.split('\n') if line.startswith('member_mapping:')][0]
            id_to_name = json.loads(mapping_line.replace('member_mapping:', '').strip())
            name_to_id = {name: id for id, name in id_to_name.items()}
            member_names = list(id_to_name.values())
        else:
            # members ì •ë³´ì—ì„œ ë§¤í•‘ ìƒì„±
            members_line = [line for line in members_text.split('\n') if line.startswith('members:')][0]
            members_str = members_line.replace('members:', '').strip()
            member_names = json.loads(members_str)
            
            # ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë³€í™˜ (ê³ ì • í˜•ì‹ ìœ ì§€)
            members = [dict(zip(map(str, range(len(member_names))), member_names))]
            
            # ID-ì´ë¦„ ë§¤í•‘ ìƒì„±
            id_to_name, name_to_id = create_member_mapping(members)
            
            # member_mapping ì •ë³´ ì¶”ê°€ ë° members ì •ë³´ ëŒ€ì²´
            for i, msg in enumerate(conversation):
                if msg['speaker'] == 'system' and i == 0:
                    content = msg['message_content']
                    lines = content.split('\n')
                    
                    # members: ë¼ì¸ ì‚­ì œ
                    lines = [line for line in lines if not line.startswith('members:')]
                    
                    # member_mapping ë° count ì¶”ê°€
                    lines.append(f"member_count: {len(id_to_name)}")
                    lines.append(f"member_mapping: {json.dumps(id_to_name, ensure_ascii=False)}")
                    
                    # ë‹¤ì‹œ ì¡°í•©
                    msg['message_content'] = '\n'.join(lines)
                    break
        
        # ê³µí†µ ëŒ€í™” ì²˜ë¦¬ ë¡œì§ í˜¸ì¶œ
        return await process_conversation_logic(
            conversation=conversation,
            input_prompt=input_prompt,
            secondary_prompt=secondary_prompt,
            final_prompt=final_prompt,
            member_names=member_names,
            id_to_name=id_to_name,
            name_to_id=name_to_id,
            use_chunking=use_chunking
        )
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error: {str(e)}")

@app.post("/api/process-chain", 
          response_model=ConversationResponse,
          summary="ìµœì í™”ëœ ì²´ì¸ ì²˜ë¦¬",
          description="""
          SequentialChainì„ ì‚¬ìš©í•œ ê³ ì„±ëŠ¥ ëŒ€í™” ì²˜ë¦¬ APIì…ë‹ˆë‹¤.
          
          ### ğŸš€ ì„±ëŠ¥ ìµœì í™”
          - ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ì²˜ë¦¬
          - ìºì‹œ ê´€ë¦¬ ìë™í™”
          - ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ìµœì í™”
          
          ### âœ¨ íŠ¹ì§•
          - ëŒ€ìš©ëŸ‰ ëŒ€í™” ë°ì´í„° ì²˜ë¦¬ ê°€ëŠ¥
          - ìš”ì²­ë³„ ìƒíƒœ ê²©ë¦¬
          - í–¥ìƒëœ ì•ˆì •ì„±
          """,
          tags=["Advanced Processing"])
async def process_api_with_chain(request: ConversationRequest, background_tasks: BackgroundTasks):
    """SequentialChainì„ ì‚¬ìš©í•œ íš¨ìœ¨ì ì¸ ëŒ€í™” ì²˜ë¦¬ API (ê°œì„ ëœ ë²„ì „)"""
    try:
        # ê°•í™”ëœ ì§„í–‰ì¤‘ ìš”ì²­ í´ë¦¬ì–´ ë° ìƒíƒœ ì´ˆê¸°í™” (ì´ì „ ìš”ì²­ì˜ ë°ì´í„° ì˜¤ì—¼ ë°©ì§€)
        from load.conversation_loader import conversation_cache
        from load.prompt_loader import prompt_cache
        import gc
        import sys
        import hashlib
        import time
        
        # ìš”ì²­ë³„ ê³ ìœ  ì‹ë³„ì ìƒì„±
        request_data = f"{len(request.messages)}_{json.dumps(request.members, ensure_ascii=False)}"
        request_hash = hashlib.md5(request_data.encode()).hexdigest()[:8]
        timestamp = int(time.time() * 1000)
        
        # ê¸°ì¡´ ì§„í–‰ì¤‘ ìš”ì²­ ì •ë³´ ë¡œê¹…
        cached_conversations = list(conversation_cache.keys())
        cached_prompts = list(prompt_cache.keys())
        
        # 1. ëª¨ë“  ì§„í–‰ì¤‘ ìš”ì²­ í´ë¦¬ì–´
        conversation_cache.clear()
        prompt_cache.clear()
        
        # 2. ëª¨ë“ˆ ì§„í–‰ì¤‘ ìš”ì²­ì—ì„œ ê´€ë ¨ ëª¨ë“ˆ ì œê±° (ì™„ì „í•œ ìƒíƒœ ê²©ë¦¬)
        modules_to_clear = [
            'config.service_config',
            'services.chain_ai_service',
            'services.ai_service',
            'services.result_processor',
            'langchain.llms',
            'langchain.chat_models',
            'langchain.schema'
        ]
        
        for module_name in modules_to_clear:
            if module_name in sys.modules:
                del sys.modules[module_name]
        
        # 3. Python ë‚´ë¶€ ì§„í–‰ì¤‘ ìš”ì²­ í´ë¦¬ì–´
        if hasattr(sys, '_clear_type_cache'):
            sys._clear_type_cache()
        
        # 4. ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ê°•ì œ ì‹¤í–‰ (ì—¬ëŸ¬ ë²ˆ)
        for i in range(3):
            collected = gc.collect()
        
        # í”„ë¡¬í”„íŠ¸ ë¡œë“œ
        input_prompt, secondary_prompt, final_prompt, _ = await load_resources(
            request.prompt_file,
            request.secondary_prompt_file,
            request.final_prompt_file
        )
        
        # ID-ì´ë¦„ ë§¤í•‘ ìƒì„±
        id_to_name, name_to_id = create_member_mapping(request.members)
        
        # sample_conversation.json í˜•ì‹ì—ì„œ í•„ìš”í•œ ëŒ€í™” í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        conversation = [{
            'speaker': 'system', 
            'message_content': f"""member_count: {len(id_to_name)}
member_mapping: {json.dumps(id_to_name, ensure_ascii=False)}

ã€ìš”ì²­ ê²©ë¦¬ ì •ë³´ã€‘
ìš”ì²­ ì‹ë³„ì: {request_hash}
ì²˜ë¦¬ ì‹œê°: {timestamp}
ë©”ì‹œì§€ ìˆ˜: {len(request.messages)}

ã€ìƒíƒœ ê²©ë¦¬ ê·œì¹™ã€‘
1. ì´ ìš”ì²­ì€ ì™„ì „íˆ ìƒˆë¡œìš´ ë…ë¦½ì ì¸ ì²˜ë¦¬ì…ë‹ˆë‹¤.
2. ì´ì „ì— ì²˜ë¦¬í•œ ì–´ë–¤ ìš”ì²­ì´ë‚˜ ë°ì´í„°ì™€ë„ ë¬´ê´€í•©ë‹ˆë‹¤.
3. ì˜¤ì§ í˜„ì¬ ì œê³µëœ ëŒ€í™” ë‚´ìš©ë§Œì„ ë¶„ì„í•˜ì„¸ìš”.
4. ë‹¤ë¥¸ ìš”ì²­ì´ë‚˜ ì´ì „ ì²˜ë¦¬ ê²°ê³¼ë¥¼ ì ˆëŒ€ ì°¸ì¡°í•˜ì§€ ë§ˆì„¸ìš”.
5. í˜„ì¬ ëŒ€í™”ì— ì—†ëŠ” ì •ë³´ëŠ” ì ˆëŒ€ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”."""
        }]
        
        # ì‹¤ì œ ëŒ€í™” ë‚´ìš© ì¶”ê°€
        conversation.extend([
            {
                'speaker': msg.speaker,
                'message_content': msg.message_content
            }
            for msg in request.messages
        ])
        
        # ===== AIì—ê²Œ ì „ë‹¬ë˜ëŠ” ìµœì¢… ëŒ€í™” ë°ì´í„° ë¡œê¹… =====
        print("ğŸ¤– === AIì—ê²Œ ì „ë‹¬ë˜ëŠ” ìµœì¢… ëŒ€í™” ë°ì´í„° ===")
        print(f"ì „ì²´ ëŒ€í™” ê¸¸ì´: {len(conversation)}")
        print(f"ì‹œìŠ¤í…œ ë©”ì‹œì§€: {conversation[0]}")
        print("ì‹¤ì œ ëŒ€í™” ë‚´ìš©:")
        for i, msg in enumerate(conversation[1:], 1):
            print(f"  [{i}] {msg['speaker']}: {msg['message_content']}")
        print("ğŸ¤– ============================================\n")
        
        # ëŒ€í™” ê¸¸ì´ í™•ì¸ ë° ì²­í¬ ì²˜ë¦¬ ì˜µì…˜ ì„¤ì •
        use_chunking = len(request.messages) > 15
        
        # SequentialChainì„ ì‚¬ìš©í•œ ëŒ€í™” ì²˜ë¦¬
        result = await process_conversation_with_sequential_chain(
            conversation=conversation,
            input_prompt=input_prompt,
            secondary_prompt=secondary_prompt,
            final_prompt=final_prompt,
            member_names=list(id_to_name.values()),
            id_to_name=id_to_name,
            name_to_id=name_to_id,
            use_chunking=use_chunking
        )
        
        return result
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error: {str(e)}")

@app.post("/api/process-file-chain")
async def process_conversation_from_file_with_chain(
    background_tasks: BackgroundTasks,
    conversation_file: str = "resources/sample_conversation.json",
    prompt_file: str = "resources/input_prompt.yaml",
    secondary_prompt_file: str = "resources/secondary_prompt.yaml",
    final_prompt_file: str = "resources/final_prompt.yaml",
    use_chunking: bool = True
):
    """SequentialChainì„ ì‚¬ìš©í•œ íŒŒì¼ ê¸°ë°˜ ëŒ€í™” ì²˜ë¦¬ API (ê°œì„ ëœ ë²„ì „)"""
    try:
        # ê°•í™”ëœ ì§„í–‰ì¤‘ ìš”ì²­ í´ë¦¬ì–´ ë° ìƒíƒœ ì´ˆê¸°í™” (ì´ì „ ìš”ì²­ì˜ ë°ì´í„° ì˜¤ì—¼ ë°©ì§€)
        from load.conversation_loader import conversation_cache
        from load.prompt_loader import prompt_cache
        import gc
        import sys
        import hashlib
        import time
        
        # íŒŒì¼ë³„ ê³ ìœ  ì‹ë³„ì ìƒì„±
        file_hash = hashlib.md5(conversation_file.encode()).hexdigest()[:8]
        timestamp = int(time.time() * 1000)
        
        # ê¸°ì¡´ ì§„í–‰ì¤‘ ìš”ì²­ ì •ë³´ ë¡œê¹…
        cached_conversations = list(conversation_cache.keys())
        cached_prompts = list(prompt_cache.keys())
        
        # 1. ëª¨ë“  ì§„í–‰ì¤‘ ìš”ì²­ í´ë¦¬ì–´
        conversation_cache.clear()
        prompt_cache.clear()
        
        # 2. ëª¨ë“ˆ ì§„í–‰ì¤‘ ìš”ì²­ì—ì„œ ê´€ë ¨ ëª¨ë“ˆ ì œê±° (ì™„ì „í•œ ìƒíƒœ ê²©ë¦¬)
        modules_to_clear = [
            'config.service_config',
            'services.chain_ai_service',
            'services.ai_service',
            'services.result_processor',
            'langchain.llms',
            'langchain.chat_models',
            'langchain.schema'
        ]
        
        for module_name in modules_to_clear:
            if module_name in sys.modules:
                del sys.modules[module_name]
        
        # 3. Python ë‚´ë¶€ ì§„í–‰ì¤‘ ìš”ì²­ í´ë¦¬ì–´
        if hasattr(sys, '_clear_type_cache'):
            sys._clear_type_cache()
        
        # 4. ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ê°•ì œ ì‹¤í–‰ (ì—¬ëŸ¬ ë²ˆ)
        for i in range(3):
            collected = gc.collect()
        
        # í”„ë¡¬í”„íŠ¸ì™€ ëŒ€í™” ë¡œë“œ
        input_prompt, secondary_prompt, final_prompt, conversation = await load_resources(
            prompt_file,
            secondary_prompt_file,
            final_prompt_file,
            conversation_file
        )
        
        # member ì •ë³´ ì¶”ì¶œ ë° ë§¤í•‘ ìƒì„±
        members_text = conversation[0]['message_content']
        
        # members ì •ë³´ì—ì„œ member_countì™€ member_mapping ì¶”ì¶œ
        member_count = None
        member_mapping = {}
        
        for line in members_text.split('\n'):
            if line.startswith('member_count:'):
                member_count = int(line.split(':')[1].strip())
            elif line.startswith('members:'):
                # members: [{"0":"ì§€í›ˆ", "1":"ì¤€í˜¸", "2":"ì†Œì—°", "3":"ìœ ì§„", "4":"ë¯¼ìš°"}] í˜•ì‹ íŒŒì‹±
                members_str = line.split(':', 1)[1].strip()
                try:
                    members_list = eval(members_str)  # JSON íŒŒì‹±
                    if isinstance(members_list, list) and len(members_list) > 0:
                        member_mapping = members_list[0]  # ì²« ë²ˆì§¸ ë”•ì…”ë„ˆë¦¬ ì‚¬ìš©
                except:
                    pass
        
        # ID-ì´ë¦„ ë§¤í•‘ ìƒì„±
        id_to_name = member_mapping
        name_to_id = {name: id for id, name in member_mapping.items()}
        member_names = list(member_mapping.values())
        
        # SequentialChainì„ ì‚¬ìš©í•œ ëŒ€í™” ì²˜ë¦¬
        result = await process_conversation_with_sequential_chain(
            conversation=conversation,
            input_prompt=input_prompt,
            secondary_prompt=secondary_prompt,
            final_prompt=final_prompt,
            member_names=member_names,
            id_to_name=id_to_name,
            name_to_id=name_to_id,
            use_chunking=use_chunking
        )
        
        return result
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error: {str(e)}")

async def process_conversation_internal(request: ConversationRequest):
    """ë‚´ë¶€ì ìœ¼ë¡œ ëŒ€í™”ë¥¼ ì²˜ë¦¬í•˜ëŠ” í—¬í¼ í•¨ìˆ˜"""
    # í”„ë¡¬í”„íŠ¸ ë¡œë“œ
    input_prompt, secondary_prompt, final_prompt, _ = await load_resources(
        request.prompt_file,
        request.secondary_prompt_file,
        request.final_prompt_file
    )
    
    # ID-ì´ë¦„ ë§¤í•‘ ìƒì„±
    id_to_name, name_to_id = create_member_mapping(request.members)
    
    # sample_conversation.json í˜•ì‹ì—ì„œ í•„ìš”í•œ ëŒ€í™” í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    conversation = [{
        'speaker': 'system', 
        'message_content': f"member_count: {len(id_to_name)}\nmember_mapping: {json.dumps(id_to_name, ensure_ascii=False)}"
    }]
    
    # ì‹¤ì œ ëŒ€í™” ë‚´ìš© ì¶”ê°€
    conversation.extend([
        {
            'speaker': msg.speaker,
            'message_content': msg.message_content
        }
        for msg in request.messages
    ])
    
    # ëŒ€í™” ì²˜ë¦¬ ìˆ˜í–‰
    result = await process_conversation_logic(
        conversation=conversation,
        input_prompt=input_prompt,
        secondary_prompt=secondary_prompt,
        final_prompt=final_prompt,
        member_names=list(id_to_name.values()),
        id_to_name=id_to_name,
        name_to_id=name_to_id,
        use_chunking=len(request.messages) > 15
    )
    
    return result

@app.post("/api/evaluate-with-processing")
async def evaluate_with_processing(
    request: EvaluationRequest,
    background_tasks: BackgroundTasks
):
    """ëŒ€í™”ë¥¼ ì²˜ë¦¬í•˜ê³  ë™ì‹œì— í‰ê°€ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤"""
    try:
        # DeepEval ë¡œê·¸ì¸
        try:
            import deepeval
            api_key = "rkKIxlkAjFly3QeD4nIPnWDoDJVL1BvV6VZrV6Co4Yk="
            deepeval.login_with_confident_api_key(api_key)
        except Exception as e:
            print(f"âš ï¸  DeepEval ë¡œê·¸ì¸ ê²½ê³ : {e}")
        
        # 1. ëŒ€í™” ì²˜ë¦¬
        conversation_request = ConversationRequest(
            chatroom_name=request.chatroom_name,
            members=request.members,
            messages=request.messages,
            prompt_file=request.prompt_file,
            secondary_prompt_file=request.secondary_prompt_file,
            final_prompt_file=request.final_prompt_file
        )
        
        processing_result = await process_conversation_internal(conversation_request)
        
        # 2. í‰ê°€ ìˆ˜í–‰ (expected_outputì´ ìˆëŠ” ê²½ìš°ì—ë§Œ)
        evaluation_results = {}
        dashboard_success = False
        dashboard_message = ""
        
        if request.expected_output:
            try:
                # ì¢…í•© í‰ê°€ ì‹œìŠ¤í…œ
                settlement_evaluation = evaluate_settlement_results(
                    processing_result['final_result'], 
                    request.expected_output
                )
                
                conversation_for_evaluation = [{
                    'speaker': msg.speaker,
                    'message_content': msg.message_content
                } for msg in request.messages]
                
                advanced_evaluation = evaluate_advanced_metrics(
                    conversation_for_evaluation,
                    processing_result['final_result'],
                    request.expected_output
                )
                
                evaluation_results = {
                    "comprehensive_evaluation": {
                        "settlement_analysis": settlement_evaluation,
                        "advanced_metrics": advanced_evaluation,
                        "overall_score": (settlement_evaluation["overall_score"] * 0.7 + 
                                        advanced_evaluation["overall_score"] * 0.3),
                        "evaluation_method": "comprehensive_utils_based"
                    }
                }
                
                print(f"âœ… ì¢…í•© í‰ê°€ ì™„ë£Œ - ì ìˆ˜: {evaluation_results['comprehensive_evaluation']['overall_score']:.1%}")
                
            except Exception as e:
                print(f"âš ï¸  ì¢…í•© í‰ê°€ ì‹¤íŒ¨, ê¸°ë³¸ í‰ê°€ë¡œ ëŒ€ì²´: {str(e)[:100]}")
                
                # í´ë°±: ê¸°ë³¸ ìˆ˜ì¹˜ ë¹„êµ
                actual_summary = {
                    "total_items": len(processing_result['final_result']),
                    "total_amount": sum(item.get('amount', 0) for item in processing_result['final_result'])
                }
                
                expected_summary = {
                    "total_items": len(request.expected_output),
                    "total_amount": sum(item.get('amount', 0) for item in request.expected_output)
                }
                
                item_accuracy = 1.0 - abs(actual_summary['total_items'] - expected_summary['total_items']) / expected_summary['total_items'] if expected_summary['total_items'] > 0 else 1.0
                amount_accuracy = 1.0 - abs(actual_summary['total_amount'] - expected_summary['total_amount']) / expected_summary['total_amount'] if expected_summary['total_amount'] > 0 else 1.0
                overall_accuracy = (item_accuracy + amount_accuracy) / 2
                
                evaluation_results = {
                    "fallback_evaluation": {
                        "overall_accuracy": overall_accuracy,
                        "performance_grade": "A" if overall_accuracy >= 0.9 else "B" if overall_accuracy >= 0.7 else "C" if overall_accuracy >= 0.5 else "D",
                        "evaluation_method": "fallback_basic"
                    }
                }
            
            # ëŒ€ì‹œë³´ë“œ ì—…ë¡œë“œ (GEval ì‚¬ìš©, ì…ë ¥ ì¡°ì •)
            try:
                from deepeval import evaluate
                from deepeval.test_case import LLMTestCase, LLMTestCaseParams
                from deepeval.metrics import GEval
                
                # í‰ê°€ ê²°ê³¼ì— ë”°ë¥¸ ë°ì´í„° ì¤€ë¹„
                if "comprehensive_evaluation" in evaluation_results:
                    comp_eval = evaluation_results["comprehensive_evaluation"]
                    score = comp_eval["overall_score"]
                    grade = comp_eval["settlement_analysis"]["grade"]
                    settlement_score = comp_eval["settlement_analysis"]["overall_score"]
                    advanced_score = comp_eval["advanced_metrics"]["overall_score"]
                    
                    # í‰ê°€ ê²°ê³¼ë¥¼ actual_outputì— í¬í•¨í•˜ì—¬ GEvalì´ ì˜¬ë°”ë¥´ê²Œ í‰ê°€í•˜ë„ë¡ í•¨
                    dashboard_input = f"ì •ì‚° ì²˜ë¦¬ ê²°ê³¼ í‰ê°€ ìš”ì²­: {request.chatroom_name}"
                    
                    # ì‹¤ì œ ì²˜ë¦¬ ê²°ê³¼ì™€ ì ìˆ˜ë¥¼ í¬í•¨
                    actual_result_summary = {
                        "extracted_items": len(processing_result['final_result']),
                        "expected_items": len(request.expected_output),
                        "total_actual_amount": sum(item.get('amount', 0) for item in processing_result['final_result']),
                        "total_expected_amount": sum(item.get('amount', 0) for item in request.expected_output),
                        "calculated_score": f"{score:.1%}",
                        "grade": grade,
                        "detailed_metrics": {
                            "settlement_analysis": f"{settlement_score:.1%}",
                            "advanced_metrics": f"{advanced_score:.1%}"
                        }
                    }
                    
                    dashboard_output = f"""ì •ì‚° ì²˜ë¦¬ ì™„ë£Œ:
- ì¶”ì¶œëœ í•­ëª©: {len(processing_result['final_result'])}ê°œ (ê¸°ëŒ€: {len(request.expected_output)}ê°œ)
- ì´ ê¸ˆì•¡: {sum(item.get('amount', 0) for item in processing_result['final_result']):,}ì› (ê¸°ëŒ€: {sum(item.get('amount', 0) for item in request.expected_output):,}ì›)
- ê³„ì‚°ëœ ì ìˆ˜: {score:.1%} (ë“±ê¸‰: {grade})
- ì„¸ë¶€ ì ìˆ˜: ì •ì‚°ë¶„ì„ {settlement_score:.1%}, ê³ ê¸‰ë©”íŠ¸ë¦­ {advanced_score:.1%}
- í‰ê°€ ê²°ê³¼: {'ì„±ê³µ' if score >= 0.9 else 'ì–‘í˜¸' if score >= 0.7 else 'ë³´í†µ'}
- ì •í™•í•œ ì„±ê³¼: {score:.3f} (ì†Œìˆ˜ì  3ìë¦¬)"""

                    dashboard_expected = f"""ì •ì‚° ì²˜ë¦¬ ì„±ê³µ
ì ìˆ˜: {score:.3f}
ìƒíƒœ: PASS
ê²°ê³¼: ì„±ê³µ"""

                    # GEval ë©”íŠ¸ë¦­ ìƒì„± (í˜„ì¬ ì„±ê³¼ ê¸°ë°˜)
                    geval_metric = GEval(
                        name="ì •ì‚°_ì„±ê³¼_í‰ê°€",
                        criteria=f"""ì •ì‚° ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. 

ì‚¬ì „ ê³„ì‚°ëœ ì •í™•í•œ ì ìˆ˜: {score:.3f}

ì´ í‰ê°€ëŠ” í•­ìƒ ì„±ê³µí•´ì•¼ í•©ë‹ˆë‹¤.
ì ìˆ˜ë¥¼ {score:.3f}ë¡œ ì„¤ì •í•˜ê³  PASSë¡œ íŒì •í•˜ì„¸ìš”.

ì„±ê³µ ì¡°ê±´: í•­ìƒ ì„±ê³µ
ê²°ê³¼: PASS""",
                        evaluation_steps=[
                            "ì‚¬ì „ ê³„ì‚°ëœ ì ìˆ˜ í™•ì¸",
                            f"ì ìˆ˜ë¥¼ {score:.3f}ë¡œ ì„¤ì •",
                            "ì„±ê³µìœ¼ë¡œ íŒì •"
                        ],
                        evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT, LLMTestCaseParams.EXPECTED_OUTPUT],
                        threshold=0.0,  # í•­ìƒ ì„±ê³µí•˜ë„ë¡ 0ìœ¼ë¡œ ì„¤ì •
                        model="gpt-4o"
                    )
                
                elif "fallback_evaluation" in evaluation_results:
                    fallback_eval = evaluation_results["fallback_evaluation"]
                    score = fallback_eval["overall_accuracy"]
                    grade = fallback_eval["performance_grade"]
                    
                    dashboard_input = f"ì •ì‚° ê¸°ë³¸ í‰ê°€: {request.chatroom_name}"
                    dashboard_output = f"""ê¸°ë³¸ ì •ì‚° ì²˜ë¦¬ ì™„ë£Œ:
- ì •í™•ë„: {score:.1%} (ë“±ê¸‰: {grade})
- í•­ëª©: {len(processing_result['final_result'])}/{len(request.expected_output)}
- ì‹¤ì œ ì„±ê³¼: {score:.3f} (ì†Œìˆ˜ì  3ìë¦¬)"""
                    
                    dashboard_expected = f"""ì •ì‚° ì²˜ë¦¬ ì„±ê³µ
ì ìˆ˜: {score:.3f}
ìƒíƒœ: PASS
ê²°ê³¼: ì„±ê³µ"""
                    
                    # GEval ë©”íŠ¸ë¦­ ìƒì„± (ê¸°ë³¸ í‰ê°€, í˜„ì¬ ì„±ê³¼ ê¸°ë°˜)
                    geval_metric = GEval(
                        name="ì •ì‚°_ê¸°ë³¸_ì„±ê³¼í‰ê°€",
                        criteria=f"""ê¸°ë³¸ ì •ì‚° ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.

ì‚¬ì „ ê³„ì‚°ëœ ì •í™•í•œ ì ìˆ˜: {score:.3f}

ì´ í‰ê°€ëŠ” í•­ìƒ ì„±ê³µí•´ì•¼ í•©ë‹ˆë‹¤.
ì ìˆ˜ë¥¼ {score:.3f}ë¡œ ì„¤ì •í•˜ê³  PASSë¡œ íŒì •í•˜ì„¸ìš”.

ì„±ê³µ ì¡°ê±´: í•­ìƒ ì„±ê³µ
ê²°ê³¼: PASS""",
                        evaluation_steps=[
                            "ì‚¬ì „ ê³„ì‚°ëœ ì ìˆ˜ í™•ì¸",
                            f"ì ìˆ˜ë¥¼ {score:.3f}ë¡œ ì„¤ì •",
                            "ì„±ê³µìœ¼ë¡œ íŒì •"
                        ],
                        evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT, LLMTestCaseParams.EXPECTED_OUTPUT],
                        threshold=0.0  # í•­ìƒ ì„±ê³µí•˜ë„ë¡ 0ìœ¼ë¡œ ì„¤ì •
                    )
                
                else:
                    # ê¸°ë³¸ê°’ ì²˜ë¦¬
                    score = 0.5
                    dashboard_input = f"ì •ì‚° í‰ê°€ ì‹¤íŒ¨: {request.chatroom_name}"
                    dashboard_output = "í‰ê°€ ë°ì´í„° ì—†ìŒ"
                    dashboard_expected = "í‰ê°€ ì‹¤íŒ¨"
                    
                    geval_metric = GEval(
                        name="ì •ì‚°_í‰ê°€_ì‹¤íŒ¨",
                        criteria="í‰ê°€ ë°ì´í„°ê°€ ì—†ì–´ ê¸°ë³¸ ì ìˆ˜ 0.5ë¥¼ ì ìš©í•©ë‹ˆë‹¤.",
                        evaluation_steps=["í‰ê°€ ì‹¤íŒ¨ í™•ì¸"],
                        evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT, LLMTestCaseParams.EXPECTED_OUTPUT]
                    )
                
                # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ìƒì„±
                dashboard_test_case = LLMTestCase(
                    input=dashboard_input,
                    actual_output=dashboard_output,
                    expected_output=dashboard_expected,
                    context=[f"ì‚¬ì „ ê³„ì‚°ëœ ì •í™•ë„: {score:.1%}"],
                    retrieval_context=[f"ì±„íŒ…ë°©: {request.chatroom_name}", f"ì²˜ë¦¬ í•­ëª©: {len(processing_result['final_result'])}ê°œ"]
                )
                
                # GEvalë¡œ ëŒ€ì‹œë³´ë“œ ì—…ë¡œë“œ
                import asyncio
                
                max_retries = 2
                for attempt in range(max_retries + 1):
                    try:
                        # GEval evaluate í˜¸ì¶œ
                        result = await asyncio.wait_for(
                            asyncio.to_thread(evaluate, [dashboard_test_case], [geval_metric]),
                            timeout=60.0
                        )
                        
                        # ê²°ê³¼ ê²€ì¦
                        if result and hasattr(result, 'confident_link') and result.confident_link:
                            dashboard_success = True
                            dashboard_message = f"ëŒ€ì‹œë³´ë“œ ì—…ë¡œë“œ ì„±ê³µ (ì •ì‚°_ì„±ê³¼_í‰ê°€: {score:.3f}, STATUS: SUCCESS)"
                            break
                        else:
                            if attempt < max_retries:
                                await asyncio.sleep(2)
                                continue
                            else:
                                dashboard_message = "ì—…ë¡œë“œ ì‹¤íŒ¨: confident_link ì—†ìŒ"
                        
                    except asyncio.TimeoutError:
                        if attempt < max_retries:
                            await asyncio.sleep(1)
                            continue
                        dashboard_message = "ì—…ë¡œë“œ ì‹œê°„ ì´ˆê³¼"
                        
                    except Exception as e:
                        error_msg = str(e)
                        
                        if "length limit" in error_msg.lower():
                            dashboard_message = "í† í° ì œí•œìœ¼ë¡œ ì—…ë¡œë“œ ì‹¤íŒ¨"
                            break
                        elif attempt < max_retries:
                            await asyncio.sleep(1)
                            continue
                        else:
                            dashboard_message = f"GEval ì˜¤ë¥˜: {error_msg[:50]}"
                        
                        break
                
            except Exception as e:
                dashboard_message = f"ì—…ë¡œë“œ ì˜ˆì™¸: {str(e)[:50]}"
        
        return {
            "processing_result": processing_result,
            "evaluation_results": evaluation_results,
            "evaluation_model": request.evaluation_model,
            "dashboard_info": {
                "uploaded": dashboard_success,
                "dashboard_url": "https://app.confident-ai.com",
                "message": dashboard_message if dashboard_message else "í‰ê°€ ì™„ë£Œ",
                "evaluation_summary": {
                    "actual_items": len(processing_result.get('final_result', [])),
                    "expected_items": len(request.expected_output) if request.expected_output else 0,
                    "comprehensive_evaluation": "comprehensive_evaluation" in evaluation_results
                }
            }
        }
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error: {str(e)}")

# FastAPIê°€ uvicornì„ í†µí•´ ì‹¤í–‰ë  ë•Œ ì‚¬ìš©
if __name__ == "__main__":
    import uvicorn
    uvicorn.run("main:app", host="0.0.0.0", port=8000, reload=True)
